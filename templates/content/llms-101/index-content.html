<h2 id="llms-101">LLMs 101</h2>
<h4 id="what-do-these-numbers-mean-in-the-names-of-models">What do these numbers mean in the names of models?</h4>
<p>For example: &quot;Vicuna-13B&quot;. The name of the model is Vicuna, and it has 13 billion parameters.</p>
<h4 id="what-is-a-parameter">What is a parameter?</h4>
<code>
{% from "partials/image-cards/image-cards.html" import imageCards %}
{% set orientation = "image-top" %}
{% set slides = [
  { "alt": "LLM Model Parameter Visualization","title": "Model", "description": "Different models are trained on different sets of data, which influences their response.", "image": "/img/ai/carousels/parameters/llm-01-model.jpg"},
  { "alt": "Max Length Parameter Visualization","title": "Max Length", "description": "Setting a limit to the number of tokens.", "image": "/img/ai/carousels/parameters/llm-02-maxlength.jpg"},
  { "alt": "Temperature Parameter Visualization","title": "Temperature", "description": "Degree of randomness. Setting a high temperature may lead to more creative responses. Setting a low temperature will result in responses that are more literal.", "image": "/img/ai/carousels/parameters/llm-03-temp.jpg"},
  { "alt": "Top P Parameter Visualization","title": "Top P (Probability)", "description": "Include only the top tokens whose percentage likelihood adds up to the specified “Top P” (e.g. 15%).", "image": "/img/ai/carousels/parameters/llm-04-top-p.jpg"},
  { "alt": "Frequency Parameter Visualization","title": "Frequency", "description": "The higher this is set, the more that repetition of tokens present in the context will be penalized in suggestions.", "image": "/img/ai/carousels/parameters/llm-05-frequency.jpg"}
]
%}

<p>{{imageCards(slides,orientation)}}</p>
</code>

<p>An LLM parameter is a value that the model learns during training. These values are adjusted through a process called backpropagation, which involves calculating the error between the model&#39;s predictions and the actual output and adjusting the parameters to minimize this error. The number of parameters in an LLM is typically very large, often numbering in the millions or even billions. These parameters capture the relationships between different words and phrases in language, allowing the model to generate human-like output and make accurate predictions. Without these parameters, an LLM would not be able to perform natural language processing tasks at a high level of accuracy.</p>
<h4 id="what-does-training-an-llm-mean">What does “training” an LLM mean?</h4>
<p>Training an LLM involves exposing it to large amounts of data so that it can learn patterns and make accurate predictions. During training, the parameters of the model are adjusted based on the input data and desired output. This process can take a significant amount of time and computational resources, but it is essential for achieving high levels of accuracy in natural language processing tasks.</p>
<h4 id="how-does-a-typical-training-run-work">How does a typical training run work?</h4>
<p>During a typical training run, the LLM is fed with a large amount of text data and corresponding targets. The model then uses these inputs to update its parameters through an optimization algorithm like stochastic gradient descent (SGD) or Adam.</p>
<p>During training, the network makes predictions on a batch of input data and calculates the loss, which is a measure of how well the predictions match the actual output. The optimizer then adjusts the weights of the network using backpropagation to minimize the loss. This process is repeated iteratively for a fixed number of epochs or until a convergence criterion is met. The process of updating the model&#39;s parameters continues iteratively until the model reaches a satisfactory level of accuracy on the training set.</p>
<p>It&#39;s worth noting that training an LLM can be a resource-intensive process, requiring significant computational power and time.</p>
<h4 id="what-is-backpropagation">What is backpropagation?</h4>
<p>Backpropagation is a process used to adjust the parameters of an LLM during training. It involves calculating the error between the model&#39;s predictions and the actual output and adjusting the parameters to minimize this error.</p>
<p>The process starts by making a forward pass through the neural network, where input data is fed into the model, and output data is generated. The difference between the predicted output and the actual output is then calculated using a loss function. This loss value is then backpropagated through the network, starting from the last layer and moving backward towards the first layer.</p>
<p>As it moves backward, each layer updates its weights based on how much it contributed to the final loss value. This process continues until all layers have updated their weights, resulting in a new set of parameters that hopefully improve the model&#39;s performance on future inputs.</p>
<h4 id="what-does-fine-tuning-an-llm-mean">What does “fine-tuning” an LLM mean?</h4>
<p>Fine-tuning an LLM involves taking a pre-trained language model and providing additional training using task-specific data sets. This process typically requires less data than training a model from scratch and can be done relatively quickly. Fine-tuning has become increasingly popular in recent years as more pre-trained models have become available.</p>
<h4 id="human-in-the-loop-approach">“Human in the loop” approach</h4>
<p>The &quot;human in the loop&quot; approach involves incorporating human feedback into machine learning models to improve their accuracy and performance. In the context of natural language processing, this might involve having humans review text generated by an LLM and provide feedback on its quality, which can then be used to fine-tune the model. Human in the loop approach is valuable because it can be used to reduce bias and errors and make usage of AI more trustworthy.</p>
<h4 id="what-does-inference-mean">What does &quot;inference&quot; mean?</h4>
<p>Inference refers to the process of using a trained machine learning model to make predictions or decisions based on new input data. In other words, it&#39;s the application of a trained model to real-world data in order to obtain useful insights or take action based on those insights. When performing inference with an LLM, the model takes in new text as input and generates output text based on what it has learned during training and fine-tuning. Inference is a critical step in the machine learning workflow, as it allows models to be used for practical applications such as chatbots, language translation, and sentiment analysis.</p>
<h4 id="is-inference-computationally-expensive">Is inference computationally expensive?</h4>
<p>YES, especially for larger models with more parameters. To address this, some LLMs use techniques such as beam search or sampling to generate output text more efficiently. Additionally, some cloud providers offer pre-trained LLMs that can be accessed via APIs for a fee, which can help reduce the computational burden of running an LLM locally.</p>
<h4 id="what-is-a-vector">What is a vector?</h4>
<p>A vector is a mathematical object that has both magnitude and direction. In the context of natural language processing, vectors are often used to represent words or phrases in a way that captures their meaning and relationship to other words. This is typically done using techniques such as word embeddings, which map each word to a high-dimensional vector based on its co-occurrence with other words in a large corpus of text.</p>
<h4 id="what-is-beam-search">What is beam search?</h4>
<p>Beam search is an algorithm used to generate output sequences from an LLM during inference. It works by maintaining a set of the top k most likely sequences at each step of the generation process, where k is known as the beam width. The algorithm then continues generating new tokens for each sequence in the set until all sequences have reached an end-of-sequence token or a maximum length has been reached. At each step, the set of possible sequences is pruned based on their likelihood according to the model&#39;s predictions, resulting in a final set of top-k output sequences.</p>
<h4 id="what-is-sampling">What is sampling?</h4>
<p>Sampling is another algorithm used to generate output sequences from an LLM during inference. Unlike beam search, which generates only the top-k most likely sequences at each step, sampling generates output tokens probabilistically based on the model&#39;s predicted probability distribution over all possible tokens at that step. This can lead to more diverse and creative output compared to beam search, but it can also result in less coherent or grammatical sentences if not properly controlled through techniques such as temperature scaling or nucleus sampling.</p>
<h4 id="what-is-temperature">What is temperature?</h4>
<p>Temperature is a technique used in LLMs to control the level of randomness and creativity in the generated output during inference. It works by scaling the predicted probability distribution over possible tokens at each step by a temperature parameter, which controls how much the probabilities are &quot;softened&quot; or spread out.</p>
<p>Lower temperatures result in more conservative and predictable output, while higher temperatures lead to more diverse and creative output. However, setting the temperature too high can also lead to nonsensical or ungrammatical sentences. Finding the optimal temperature for a given task or application often requires experimentation and fine-tuning.</p>
